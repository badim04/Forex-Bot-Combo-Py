{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eb782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03067a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b877fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your dataset in a CSV file named 'Binance_BTCUSDT_d_sorted.csv'\n",
    "data = pd.read_csv('Binance_BTCUSDT_d_sorted.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629835dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort data by Date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create additional features based on technical indicators\n",
    "def add_indicators(df):\n",
    "    df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['EMA_5'] = df['Close'].ewm(span=21, adjust=False).mean()\n",
    "    df['EMA_10'] = df['Close'].ewm(span=34, adjust=False).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'], 14)\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = compute_macd(df['Close'])\n",
    "    df['Bollinger_Upper'], df['Bollinger_Lower'] = compute_bollinger_bands(df['Close'])\n",
    "    df['ATR'] = compute_atr(df['High'], df['Low'], df['Close'], 14)\n",
    "    df['Volume_BTC'] = df['Volume BTC']\n",
    "    df['Volume_USDT'] = df['Volume USDT']\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period):\n",
    "    delta = series.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = series.ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_period, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "    hist = macd - signal\n",
    "    return macd, signal, hist\n",
    "\n",
    "def compute_bollinger_bands(series, window=20, no_of_std=2):\n",
    "    rolling_mean = series.rolling(window).mean()\n",
    "    rolling_std = series.rolling(window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * no_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * no_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def compute_atr(high, low, close, period):\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    true_range = pd.DataFrame({'TR1': tr1, 'TR2': tr2, 'TR3': tr3}).max(axis=1)\n",
    "    return true_range.rolling(window=period).mean()\n",
    "\n",
    "data = add_indicators(data)\n",
    "\n",
    "# Drop rows with NaN values created by rolling windows\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaa490-321b-4d86-9315-c1c71c7e41c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59734626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X = data[['SMA_5', 'SMA_10', 'EMA_5', 'EMA_10', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Volume_BTC', 'Volume_USDT']]\n",
    "y = data['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4c12f-2144-43e4-a74a-5b8d12f23bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM [samples, timesteps, features]\n",
    "X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False))\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd1be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Reshape input data for LSTM [samples, timesteps, features]\n",
    "X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Input(shape=(1, X_train.shape[1])))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False))\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38b65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape input data for CNN [samples, timesteps, features]\n",
    "X_train_cnn = np.reshape(X_train.values, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test.values, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units=50, activation='relu'))\n",
    "model_cnn.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=50, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a81a67-7c53-4407-a300-ea497c339ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
    "\n",
    "# Reshape input data for CNN [samples, timesteps, features]\n",
    "X_train_cnn = np.reshape(X_train.values, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test.values, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Input(shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units=50, activation='relu'))\n",
    "model_cnn.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=50, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_xgb = model_xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model\n",
    "predictions_lstm = model_lstm.predict(X_test_lstm)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, predictions_lstm))\n",
    "print('LSTM RMSE:', rmse_lstm)\n",
    "\n",
    "# Evaluate CNN model\n",
    "predictions_cnn = model_cnn.predict(X_test_cnn)\n",
    "rmse_cnn = np.sqrt(mean_squared_error(y_test, predictions_cnn))\n",
    "print('CNN RMSE:', rmse_cnn)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "print('XGBoost RMSE:', rmse_xgb)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "#plt.plot(predictions_lstm, label='LSTM Predictions')\n",
    "#plt.plot(predictions_cnn, label='CNN Predictions')\n",
    "plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import google.protobuf\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Protobuf version:\", google.protobuf.__version__)\n",
    "\n",
    "\n",
    "# NumPy version: 1.19.5\n",
    "# TensorFlow version: 2.6.0\n",
    "# Protobuf version: 3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import google.protobuf\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Protobuf version:\", google.protobuf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e24bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6331b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise predicting the daily change percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your dataset in a CSV file named 'Binance_BTCUSDT_d_sorted.csv'\n",
    "data = pd.read_csv('Bittrex_BTCUSDT_1h_sorted.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort data by Date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate daily change and percentage change\n",
    "data['Daily_Change'] = data['Close'].diff()\n",
    "data['Daily_Change_Percentage'] = data['Daily_Change'] / data['Close'].shift(1) * 100\n",
    "\n",
    "# Drop the first row with NaN values from the shift operation\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Add direction column\n",
    "data['Direction'] = data['Daily_Change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Create additional features based on technical indicators\n",
    "def add_indicators(df):\n",
    "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['SMA_200'] = df['Close'].rolling(window=200).mean()\n",
    "    df['EMA_21'] = df['Close'].ewm(span=21, adjust=False).mean()\n",
    "    df['EMA_34'] = df['Close'].ewm(span=34, adjust=False).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'], 14)\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = compute_macd(df['Close'])\n",
    "    df['Bollinger_Upper'], df['Bollinger_Lower'] = compute_bollinger_bands(df['Close'])\n",
    "    df['ATR'] = compute_atr(df['High'], df['Low'], df['Close'], 14)\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period):\n",
    "    delta = series.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = series.ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_period, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "    hist = macd - signal\n",
    "    return macd, signal, hist\n",
    "\n",
    "def compute_bollinger_bands(series, window=20, no_of_std=2):\n",
    "    rolling_mean = series.rolling(window).mean()\n",
    "    rolling_std = series.rolling(window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * no_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * no_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def compute_atr(high, low, close, period):\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    true_range = pd.DataFrame({'TR1': tr1, 'TR2': tr2, 'TR3': tr3}).max(axis=1)\n",
    "    return true_range.rolling(window=period).mean()\n",
    "\n",
    "data = add_indicators(data)\n",
    "\n",
    "# Drop rows with NaN values created by rolling windows\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29b2fb-1ca2-499a-b1d3-9effadc6e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ae0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X = data[['Low', 'High', 'SMA_50', 'SMA_200', 'EMA_21', 'EMA_34', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Daily_Change_Percentage']]\n",
    "#y = data['Daily_Change_Percentage']\n",
    "y = data['Daily Change']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7eda48-61b0-4e75-904f-4c4770615bff",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input\n",
    "\n",
    " Reshape input data for CNN [samples, timesteps, features]\n",
    "X_train_cnn = np.reshape(X_train.values, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test.values, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    " Define CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Input(shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units=50, activation='relu'))\n",
    "model_cnn.add(Dense(units=1))\n",
    "\n",
    " Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    " Train the model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=50, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bec4a-1d84-4ee1-b97b-146cf07c34fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399de3b2-4e58-46b3-9636-7d92860ac459",
   "metadata": {},
   "source": [
    " Evaluate LSTM model\n",
    "predictions_cnn = model_cnn.predict(X_test_cnn)\n",
    "rmse_cnn = np.sqrt(mean_squared_error(y_test, predictions_cnn))\n",
    "print('CNN RMSE:', rmse_cnn)\n",
    "\n",
    " Plot training & validation loss values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    " Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "plt.plot(predictions_cnn, label='CNN Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e2a2e-8d91-45df-9ea3-630e7166395f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e99386-0340-41be-84b5-0e1aa3aab547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Reshape input data for XGBoost\n",
    "X_train_xgb = X_train.values\n",
    "X_test_xgb = X_test.values\n",
    "\n",
    "# Define XGBoost model with max_depth\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=2000, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train_xgb, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138a8c4-9f1d-4519-8a2d-15086d6f54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost model\n",
    "predictions_xgb = model_xgb.predict(X_test_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "print('XGBoost RMSE:', rmse_xgb)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "#plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d05ef6-b01f-4c14-b875-0d669d5398c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "results_df = pd.DataFrame({\n",
    "    'True Values': y_test.values,\n",
    "    'Predictions': predictions_xgb\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'xgboost_predictions1.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(file_path, index=False)\n",
    "print(f'Results saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f31f9-958d-43d1-a471-872138192c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape input data for XGBoost\n",
    "X_train_xgb = X_train.values\n",
    "X_test_xgb = X_test.values\n",
    "\n",
    "# Define XGBoost model\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=2000, learning_rate=0.01, max_depth=5)\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "predictions_xgb = model_xgb.predict(X_test_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, predictions_xgb))\n",
    "print('XGBoost RMSE:', rmse_xgb)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080be894-0a75-4599-ad20-7185516b9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "results_df = pd.DataFrame({\n",
    "    'True Values': y_test.values,\n",
    "    'Predictions': predictions_xgb\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'xgboost_predictions2nd.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(file_path, index=False)\n",
    "print(f'Results saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739d5b0-443d-4ad4-9e59-14fecc5dc6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fa8ce-b719-4e3b-8390-d5202d72f07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f11735-3cf1-4a77-952a-2cae9ef8660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data[['Low', 'High', 'SMA_50', 'SMA_200', 'EMA_21', 'EMA_34', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Daily_Change', 'Daily_Change_Percentage']]\n",
    "y = data['Direction']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape input data for XGBoost\n",
    "X_train_xgb = X_train.values\n",
    "X_test_xgb = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233761e-9b5d-4401-8106-e2dda2a42a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost model with max_depth for classification\n",
    "model_xgb = xgb.XGBClassifier(n_estimators=2000, learning_rate=0.1, max_depth=5, objective='binary:logistic')\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train_xgb, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21471239-c0ff-464c-bf84-285ed94c24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost model\n",
    "predictions_xgb_proba = model_xgb.predict_proba(X_test_xgb)[:, 1]\n",
    "predictions_xgb = (predictions_xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, predictions_xgb)\n",
    "print('XGBoost Accuracy:', accuracy_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabce35-56d4-4a09-a478-27575bd752a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68acea-297b-48f0-8873-cb2afd5f7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "results_df = pd.DataFrame({\n",
    "    'True Values': y_test.values,\n",
    "    'Predictions': predictions_xgb\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'xgboost_predictions1.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(file_path, index=False)\n",
    "print(f'Results saved to {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dab76f-f69d-4904-977f-f2344f049c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a7d94-195b-4751-8515-b7a4f180b17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b06c11-e4d9-477f-a762-dee6bcea8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data[['Low', 'High', 'SMA_50', 'SMA_200', 'EMA_21', 'EMA_34', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Daily_Change', 'Daily_Change_Percentage']].values\n",
    "y = data['Direction'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape input data for CNN\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_cnn, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_cnn_proba = model_cnn.predict(X_test_cnn)\n",
    "predictions_cnn = (predictions_cnn_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_cnn = accuracy_score(y_test, predictions_cnn)\n",
    "print('CNN Accuracy:', accuracy_cnn)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(predictions_cnn, label='CNN Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df_cnn = pd.DataFrame({\n",
    "    'True Values': y_test,\n",
    "    'Predictions': predictions_cnn.flatten()\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path_cnn = 'cnn_predictions.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df_cnn.to_excel(file_path_cnn, index=False)\n",
    "print(f'Results saved to {file_path_cnn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d406c9-38de-46a5-add1-719c07ee2401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf27ee6-4ac2-420a-a81a-d100a089b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data[['Low', 'High', 'SMA_50', 'SMA_200', 'EMA_21', 'EMA_34', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Daily_Change', 'Daily_Change_Percentage']].values\n",
    "y = data['Direction'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_lstm_proba = model_lstm.predict(X_test_lstm)\n",
    "predictions_lstm = (predictions_lstm_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_lstm = accuracy_score(y_test, predictions_lstm)\n",
    "print('LSTM Accuracy:', accuracy_lstm)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(predictions_lstm, label='LSTM Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df_lstm = pd.DataFrame({\n",
    "    'True Values': y_test,\n",
    "    'Predictions': predictions_lstm.flatten()\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path_lstm = 'lstm_predictions.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df_lstm.to_excel(file_path_lstm, index=False)\n",
    "print(f'Results saved to {file_path_lstm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c3807-df4d-4e1b-a9b4-51100944b296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62d578-5927-4c04-8c9f-2db0c5f73820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it is a previous day to predict next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d7854-e16c-4b8c-92a2-ab481eace949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463eb6a-785c-4ae5-82c5-8b1fb075bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your dataset in a CSV file named 'Binance_BTCUSDT_d_sorted.csv'\n",
    "data = pd.read_csv('Bittrex_BTCUSDT_1h_sorted.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b27193-ce7b-4ca5-9419-5dae7bc9a4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41c827-6199-4c7b-a910-b898e16a4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort data by Date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate daily change and percentage change\n",
    "data['Daily_Change'] = data['Close'].diff()\n",
    "data['Daily_Change_Percentage'] = data['Daily_Change'] / data['Close'].shift(1) * 100\n",
    "\n",
    "# Drop the first row with NaN values from the shift operation\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Add direction column\n",
    "data['Direction'] = data['Daily_Change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Create additional features based on technical indicators\n",
    "def add_indicators(df):\n",
    "    df['SMA_21'] = df['Close'].rolling(window=21).mean()\n",
    "    df['SMA_55'] = df['Close'].rolling(window=55).mean()\n",
    "    df['EMA_13'] = df['Close'].ewm(span=34, adjust=False).mean()\n",
    "    df['EMA_89'] = df['Close'].ewm(span=89, adjust=False).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'], 14)\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = compute_macd(df['Close'])\n",
    "    df['Bollinger_Upper'], df['Bollinger_Lower'] = compute_bollinger_bands(df['Close'])\n",
    "    df['ATR'] = compute_atr(df['High'], df['Low'], df['Close'], 14)\n",
    "    df['Stochastic_K'], df['Stochastic_D'] = compute_stochastic(df['High'], df['Low'], df['Close'])\n",
    "    df['Williams_%R'] = compute_williams_r(df['High'], df['Low'], df['Close'])\n",
    "    df['PSAR'] = psar(df['High'], df['Low'])\n",
    "    df['CCI'] = compute_cci(df['High'], df['Low'], df['Close'], 20)\n",
    "    ichimoku = compute_ichimoku(df['High'], df['Low'], df['Close'])\n",
    "    df['Ichimoku_Conversion_Line'] = ichimoku['Conversion_Line']\n",
    "    df['Ichimoku_Base_Line'] = ichimoku['Base_Line']\n",
    "    df['Ichimoku_Leading_Span_A'] = ichimoku['Leading_Span_A']\n",
    "    df['Ichimoku_Leading_Span_B'] = ichimoku['Leading_Span_B']\n",
    "    df['Ichimoku_Lagging_Span'] = ichimoku['Lagging_Span']\n",
    "    df['VWAP'] = compute_vwap(df['Close'], df['Volume'])\n",
    "    #df['OBV'] = compute_obv(df['Close'], df['Volume'])\n",
    "    df['CMF'] = compute_cmf(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "    df['TSI'] = compute_tsi(df['Close'])\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period):\n",
    "    delta = series.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = series.ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_period, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "    hist = macd - signal\n",
    "    return macd, signal, hist\n",
    "\n",
    "def compute_bollinger_bands(series, window=21, no_of_std=2):\n",
    "    rolling_mean = series.rolling(window).mean()\n",
    "    rolling_std = series.rolling(window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * no_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * no_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def compute_atr(high, low, close, period):\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    true_range = pd.DataFrame({'TR1': tr1, 'TR2': tr2, 'TR3': tr3}).max(axis=1)\n",
    "    return true_range.rolling(window=period).mean()\n",
    "\n",
    "def compute_stochastic(high, low, close, k_period=14, d_period=3):\n",
    "    low_min = low.rolling(window=k_period).min()\n",
    "    high_max = high.rolling(window=k_period).max()\n",
    "    stoch_k = 100 * ((close - low_min) / (high_max - low_min))\n",
    "    stoch_d = stoch_k.rolling(window=d_period).mean()\n",
    "    return stoch_k, stoch_d\n",
    "\n",
    "def compute_williams_r(high, low, close, period=14):\n",
    "    highest_high = high.rolling(window=period).max()\n",
    "    lowest_low = low.rolling(window=period).min()\n",
    "    williams_r = (highest_high - close) / (highest_high - lowest_low) * -100\n",
    "    return williams_r\n",
    "    \n",
    "def psar(high, low):\n",
    "    psar = [0] * len(high)\n",
    "    psar[0] = low.iloc[0]  # Use .iloc instead of direct indexing\n",
    "    acceleration = 0.02\n",
    "    max_acceleration = 0.2\n",
    "    for i in range(1, len(high)):\n",
    "        if high.iloc[i] > psar[i-1]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = psar[i-1] + acceleration * (high.iloc[i] - psar[i-1])  # Use .iloc instead of direct indexing\n",
    "        else:\n",
    "            psar[i] = psar[i-1] - acceleration * (psar[i-1] - low.iloc[i])  # Use .iloc instead of direct indexing\n",
    "        if psar[i] > high.iloc[i]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = high.iloc[i]\n",
    "        elif psar[i] < low.iloc[i]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = low.iloc[i]\n",
    "        acceleration = min(max_acceleration, acceleration + 0.02)\n",
    "    return psar\n",
    "    \n",
    "\n",
    "def compute_cci(high, low, close, period=20):\n",
    "    tp = (high + low + close) / 3\n",
    "    tp_sma = tp.rolling(window=period).mean()\n",
    "    mad = tp.rolling(window=period).apply(lambda x: np.mean(np.abs(x - np.mean(x))))\n",
    "    cci = (tp - tp_sma) / (0.015 * mad)\n",
    "    return cci\n",
    "\n",
    "def compute_ichimoku(high, low, close):\n",
    "    nine_period_high = high.rolling(window=9).max()\n",
    "    nine_period_low = low.rolling(window=9).min()\n",
    "    period26_high = high.rolling(window=26).max()\n",
    "    period26_low = low.rolling(window=26).min()\n",
    "    period52_high = high.rolling(window=52).max()\n",
    "    period52_low = low.rolling(window=52).min()\n",
    "    ichimoku_cloud = {\n",
    "        'Conversion_Line': (nine_period_high + nine_period_low) / 2,\n",
    "        'Base_Line': (period26_high + period26_low) / 2,\n",
    "        'Leading_Span_A': ((nine_period_high + nine_period_low) / 2 + (period26_high + period26_low) / 2) / 2,\n",
    "        'Leading_Span_B': (period52_high + period52_low) / 2,\n",
    "        'Lagging_Span': close.shift(-26)\n",
    "    }\n",
    "    return ichimoku_cloud\n",
    "\n",
    "def compute_vwap(close, volume):\n",
    "    return (close * volume).cumsum() / volume.cumsum()\n",
    "\n",
    "def compute_obv(close, volume):\n",
    "    obv = volume.copy()\n",
    "    obv[1:] = np.where(close[1:] > close[:-1], volume[1:], np.where(close[1:] < close[:-1], -volume[1:], 0))\n",
    "    return obv.cumsum()\n",
    "\n",
    "def compute_cmf(high, low, close, volume, period=20):\n",
    "    mfv = ((close - low) - (high - close)) / (high - low) * volume\n",
    "    cmf = mfv.rolling(window=period).sum() / volume.rolling(window=period).sum()\n",
    "    return cmf\n",
    "\n",
    "def compute_tsi(close, r=25, s=13):\n",
    "    m25 = close.diff(1)\n",
    "    abs_m25 = abs(m25)\n",
    "    m25s = m25.ewm(span=r, adjust=False).mean()\n",
    "    abs_m25s = abs_m25.ewm(span=r, adjust=False).mean()\n",
    "    m25s = m25s.ewm(span=s, adjust=False).mean()\n",
    "    abs_m25s = abs_m25s.ewm(span=s, adjust=False).mean()\n",
    "    tsi = m25s / abs_m25s * 100\n",
    "    return tsi\n",
    "\n",
    "# Apply the indicators\n",
    "data = add_indicators(data)\n",
    "\n",
    "# Drop rows with NaN values created by rolling windows\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce77c6-fb1d-406a-8925-2d4e3dd781fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PSAR to binary values based on Close prices\n",
    "data['PSAR_Binary'] = data.apply(lambda row: 1 if row['PSAR'] > row['Close'] else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed911-e63a-44e2-8531-c761a80ddc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = 'data_indicators2nd.csv'\n",
    "\n",
    "# Save to Excel\n",
    "data.to_csv(file_path, index=True)\n",
    "print(f'Results saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2606901-a7fd-45a0-b146-2b2153d18db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the features by 1 day\n",
    "\n",
    "X = data[['SMA_21', 'SMA_55', 'EMA_13', 'EMA_89', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', \n",
    "          'Bollinger_Lower', 'ATR', 'Daily_Change', 'Daily_Change_Percentage', 'Stochastic_K', 'Stochastic_D', \n",
    "          'Williams_%R', 'CCI', 'Ichimoku_Conversion_Line', 'Ichimoku_Base_Line', \n",
    "          'Ichimoku_Leading_Span_A', 'Ichimoku_Leading_Span_B', 'Ichimoku_Lagging_Span', 'VWAP', 'CMF', 'TSI', 'PSAR', 'PSAR_Binary']].shift(1)\n",
    "y = data['Direction']\n",
    "\n",
    "\n",
    "# Drop the first row since it will have NaN values due to shifting\n",
    "X = X.dropna()\n",
    "y = y.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e08552-1ad6-47b3-bde6-2dca3e4682ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "# Reshape input data for XGBoost\n",
    "X_train_xgb = X_train.values\n",
    "X_test_xgb = X_test.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bdd69-0bbe-43b5-9716-1a94fa96cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost model for classification\n",
    "model_xgb = xgb.XGBClassifier(n_estimators=2000, learning_rate=0.01, max_depth=1, objective='binary:logistic')\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "predictions_xgb_proba = model_xgb.predict_proba(X_test_xgb)[:, 1]\n",
    "predictions_xgb = (predictions_xgb_proba >= 0.5).astype(int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee17c0-237b-435f-8d7b-440cf3cdd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, predictions_xgb)\n",
    "print('XGBoost Accuracy:', accuracy_xgb)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "#plt.figure(figsize=(14, 7))\n",
    "#plt.plot(y_test.values, label='True')\n",
    "#plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e2374-bae6-4728-95cb-f5c3cf4c882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "results_df_xgb = pd.DataFrame({\n",
    "    'True Values': y_test.values,\n",
    "    'Predictions': predictions_xgb\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path_xgb = 'xgboost_predictionsfinal.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df_xgb.to_excel(file_path_xgb, index=False)\n",
    "print(f'Results saved to {file_path_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a309c39-e3bb-4eb9-9eb5-1d1eefd3925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7018a-bd1c-4763-9a64-d3972857b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other models\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2be54-bdc2-4f8c-985d-da3f9aa42711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define and train Logistic Regression model with increased iterations and different solver\n",
    "log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = log_reg.predict_proba(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "predictions_log_reg = (probs[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_log_reg = accuracy_score(y_test, predictions_log_reg)\n",
    "print('Logistic Regression Accuracy:', accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d568be-6aba-441b-acae-9c9288247b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Record start time and train Logistic Regression\n",
    "start_time = time.time()\n",
    "log_reg = LogisticRegression(max_iter=2000, solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_time = time.time() - start_time\n",
    "\n",
    "# Record start time and train Decision Tree Classifier\n",
    "start_time = time.time()\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_clf_time = time.time() - start_time\n",
    "\n",
    "# Record start time and train Random Forest Classifier\n",
    "start_time = time.time()\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_clf_time = time.time() - start_time\n",
    "\n",
    "# Record start time and train XGBoost Classifier\n",
    "start_time = time.time()\n",
    "xgb_clf = XGBClassifier(n_estimators=2000, learning_rate=0.01, max_depth=1, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', random_state=42, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_clf_time = time.time() - start_time\n",
    "\n",
    "# Record start time and train Gradient Boosting Classifier\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1, max_depth=1, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_clf_time = time.time() - start_time\n",
    "\n",
    "# Predict on test data\n",
    "log_reg_pred = log_reg.predict(X_test)\n",
    "dt_clf_pred = dt_clf.predict(X_test)\n",
    "rf_clf_pred = rf_clf.predict(X_test)\n",
    "xgb_clf_pred = xgb_clf.predict(X_test)\n",
    "gb_clf_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "log_reg_acc = accuracy_score(y_test, log_reg_pred)\n",
    "dt_clf_acc = accuracy_score(y_test, dt_clf_pred)\n",
    "rf_clf_acc = accuracy_score(y_test, rf_clf_pred)\n",
    "xgb_clf_acc = accuracy_score(y_test, xgb_clf_pred)\n",
    "gb_clf_acc = accuracy_score(y_test, gb_clf_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_acc)\n",
    "print(\"Decision Tree Classifier Accuracy:\", dt_clf_acc)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_clf_acc)\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_clf_acc)\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", gb_clf_acc)\n",
    "\n",
    "# Print training times\n",
    "print(\"\\nTraining Times (in seconds):\")\n",
    "print(\"Logistic Regression Training Time:\", log_reg_time)\n",
    "print(\"Decision Tree Classifier Training Time:\", dt_clf_time)\n",
    "print(\"Random Forest Classifier Training Time:\", rf_clf_time)\n",
    "print(\"XGBoost Classifier Training Time:\", xgb_clf_time)\n",
    "print(\"Gradient Boosting Classifier Training Time:\", gb_clf_time)\n",
    "\n",
    "# Combine predictions using weighted majority vote\n",
    "log_reg_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "dt_clf_proba = dt_clf.predict_proba(X_test)[:, 1]\n",
    "rf_clf_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "xgb_clf_proba = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "gb_clf_proba = gb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "combined_proba = (log_reg_proba + dt_clf_proba + rf_clf_proba + xgb_clf_proba + gb_clf_proba) / 5\n",
    "combined_pred = (combined_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate and print combined accuracy\n",
    "combined_acc = accuracy_score(y_test, combined_pred)\n",
    "print(\"Combined Classifier Accuracy:\", combined_acc)\n",
    "\n",
    "# Display the percentage confidence scores for each classifier\n",
    "print(\"\\nLogistic Regression Confidence Scores:\")\n",
    "print(log_reg_proba)\n",
    "\n",
    "print(\"\\nDecision Tree Classifier Confidence Scores:\")\n",
    "print(dt_clf_proba)\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Confidence Scores:\")\n",
    "print(rf_clf_proba)\n",
    "\n",
    "print(\"\\nXGBoost Classifier Confidence Scores:\")\n",
    "print(xgb_clf_proba)\n",
    "\n",
    "print(\"\\nGradient Boosting Classifier Confidence Scores:\")\n",
    "print(gb_clf_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbd96c-186c-4b54-a657-c79b4349b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "log_reg_acc = accuracy_score(y_test, log_reg_pred)\n",
    "dt_clf_acc = accuracy_score(y_test, dt_clf_pred)\n",
    "rf_clf_acc = accuracy_score(y_test, rf_clf_pred)\n",
    "xgb_clf_acc = accuracy_score(y_test, xgb_clf_pred)\n",
    "gb_clf_acc = accuracy_score(y_test, gb_clf_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_acc)\n",
    "print(\"Decision Tree Classifier Accuracy:\", dt_clf_acc)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_clf_acc)\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_clf_acc)\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", gb_clf_acc)\n",
    "\n",
    "# Print training times\n",
    "print(\"\\nTraining Times (in seconds):\")\n",
    "print(\"Logistic Regression Training Time:\", log_reg_time)\n",
    "print(\"Decision Tree Classifier Training Time:\", dt_clf_time)\n",
    "print(\"Random Forest Classifier Training Time:\", rf_clf_time)\n",
    "print(\"XGBoost Classifier Training Time:\", xgb_clf_time)\n",
    "print(\"Gradient Boosting Classifier Training Time:\", gb_clf_time)\n",
    "\n",
    "# Combine predictions using weighted majority vote\n",
    "combined_proba = (log_reg_proba + dt_clf_proba + rf_clf_proba + xgb_clf_proba + gb_clf_proba) / 5\n",
    "combined_pred = (combined_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate and print combined accuracy\n",
    "combined_acc = accuracy_score(y_test, combined_pred)\n",
    "print(\"Combined Classifier Accuracy:\", combined_acc)\n",
    "\n",
    "# Create a DataFrame to store true values, predictions, and confidence scores\n",
    "results_df = pd.DataFrame({\n",
    "    'True Values': y_test,\n",
    "    'Logistic Regression Prediction': log_reg_pred,\n",
    "    'Logistic Regression Confidence': log_reg_proba,\n",
    "    'Decision Tree Prediction': dt_clf_pred,\n",
    "    'Decision Tree Confidence': dt_clf_proba,\n",
    "    'Random Forest Prediction': rf_clf_pred,\n",
    "    'Random Forest Confidence': rf_clf_proba,\n",
    "    'XGBoost Prediction': xgb_clf_pred,\n",
    "    'XGBoost Confidence': xgb_clf_proba,\n",
    "    'Gradient Boosting Prediction': gb_clf_pred,\n",
    "    'Gradient Boosting Confidence': gb_clf_proba,\n",
    "    'Combined Prediction': combined_pred,\n",
    "    'Combined Confidence': combined_proba\n",
    "})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "results_df.to_csv('model_predictions_with_confidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58680871-d67e-4a9e-8301-283b513be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {\n",
    "    'True Values': y_test.values,\n",
    "    'Logistic Regression': log_reg_pred,\n",
    "    'Decision Tree Classifier': dt_clf_pred,\n",
    "    'Random Forest Classifier': rf_clf_pred,\n",
    "    'GradB': gb_clf_pred,\n",
    "    'XGBoost Classifier': xgb_clf_pred\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'all_models_predictions3final3.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(file_path, index=False)\n",
    "print(f'Results saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa869ca0-7866-4d8b-b225-c69fd1c7ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dictionary\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate overall correlation\n",
    "correlation = results_df.corr()['True Values'][1:]\n",
    "\n",
    "print(\"Overall Correlation:\")\n",
    "print(correlation)\n",
    "\n",
    "# Calculate absolute differences per row\n",
    "abs_diff = results_df.iloc[:, 1:].apply(lambda col: (results_df['True Values'] - col).abs())\n",
    "\n",
    "# Find the model with the smallest difference per row\n",
    "min_diff_model = abs_diff.idxmin(axis=1)\n",
    "\n",
    "# Combine results with the original DataFrame\n",
    "result_df = results_df.copy()\n",
    "result_df['Min Diff Model'] = min_diff_model\n",
    "\n",
    "print(\"\\nPer-Row Best Model:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758b9af-8d5f-448d-9ffb-73cbb61d4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = results_df.corr()\n",
    "\n",
    "# Convert correlation values to percentages\n",
    "correlation_matrix_percentage = correlation_matrix * 100\n",
    "\n",
    "print(\"Correlation Matrix in Percentages:\")\n",
    "print(correlation_matrix_percentage)\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_percentage, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'format': '%.0f%%'})\n",
    "plt.title('Correlation Matrix in Percentages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa763bd-7ba1-4752-bac8-cee74c8ed437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Calculate individual accuracies\n",
    "accuracies = {}\n",
    "for col in results_df.columns[1:]:\n",
    "    accuracies[col] = (results_df['True Values'] == results_df[col]).mean()\n",
    "    \n",
    "print(\"Individual Accuracies:\")\n",
    "for model, accuracy in accuracies.items():\n",
    "    print(f\"{model}: {accuracy:.2f}\")\n",
    "\n",
    "# Function to calculate accuracy of majority voting\n",
    "def majority_vote_accuracy(results_df, model_combination):\n",
    "    votes = results_df[list(model_combination)].mode(axis=1)[0]\n",
    "    return (votes == results_df['True Values']).mean()\n",
    "\n",
    "# Get all combinations of 2 and 3 models\n",
    "models = results_df.columns[1:]\n",
    "combinations_2 = list(itertools.combinations(models, 3))\n",
    "combinations_3 = list(itertools.combinations(models, 3))\n",
    "\n",
    "# Calculate accuracies for all combinations\n",
    "combo_accuracies = {}\n",
    "for combo in combinations_2 + combinations_3:\n",
    "    accuracy = majority_vote_accuracy(results_df, combo)\n",
    "    combo_accuracies[combo] = accuracy\n",
    "\n",
    "# Find the best combination\n",
    "best_combination = max(combo_accuracies, key=combo_accuracies.get)\n",
    "best_accuracy = combo_accuracies[best_combination]\n",
    "\n",
    "print(\"\\nBest Combination:\")\n",
    "print(best_combination)\n",
    "print(f\"Accuracy: {best_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e757595-639f-423f-82e7-af45b8e4a0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20b34b-05cb-4d01-bb0f-5066f2989575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train Symbolic Regression model\n",
    "sym_reg = SymbolicRegressor(population_size=500, generations=50)\n",
    "sym_reg.fit(X_train, y_train)\n",
    "predictions_sym = sym_reg.predict(X_test)\n",
    "accuracy_sym = accuracy_score(y_test, predictions_sym)\n",
    "print('Symbolic Regression Accuracy:', accuracy_sym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245d084-baf5-4c9e-a990-9808393d4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true vs predicted values for each model\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.values, label='True')\n",
    "plt.plot(predictions_xgb, label='XGBoost Predictions')\n",
    "plt.plot(predictions_mlr, label='MLR Predictions')\n",
    "plt.plot(predictions_sym, label='Symbolic Regression Predictions')\n",
    "plt.plot(predictions_eml, label='EML Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa52f3-bb84-4d02-a298-17acc132cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39325f4c-e8e9-4764-afff-5e9a3816d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb7035-f8d9-47ae-8103-09dd3d701ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417c577-9b52-4675-9f99-53e4763711b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7e41b-f79f-45c5-a3ed-b77c802c0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next 24 days using the XGBoost model\n",
    "def predict_next_days_xgb(model, X_last_day, num_days=24):\n",
    "    predictions = []\n",
    "    current_input = X_last_day.reshape((1, X_last_day.shape[0]))\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        prediction_proba = model.predict_proba(current_input)[:, 1]\n",
    "        prediction = (prediction_proba >= 0.5).astype(int)\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "        # Update current_input with the latest prediction for the next day's input\n",
    "        current_input = np.roll(current_input, -1)\n",
    "        current_input[0, -1] = prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Get the last day's data from the training set to start the predictions\n",
    "X_last_day_xgb = X_train.iloc[-1].values\n",
    "\n",
    "# Predict the next 24 days using XGBoost model\n",
    "next_24_days_predictions_xgb = predict_next_days_xgb(model_xgb, X_last_day_xgb, num_days=24)\n",
    "\n",
    "print('Next 24 days predictions (XGBoost):', next_24_days_predictions_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119d189-3b7a-40c8-b55b-887db7502d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef163e50-b932-4c73-871a-396eeacd16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data[['Low', 'High', 'SMA_50', 'SMA_200', 'EMA_21', 'EMA_34', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Daily_Change', 'Daily_Change_Percentage']].shift(1)\n",
    "y = data['Direction']\n",
    "\n",
    "# Drop the first row since it will have NaN values due to shifting\n",
    "X = X.dropna()\n",
    "y = y.iloc[1:]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': randint(200, 1000, 2000),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'max_depth': randint(1, 5, 10),\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'colsample_bytree': uniform(0.7, 0.3),\n",
    "    'gamma': uniform(0, 0.5)\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model_xgb = xgb.XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    model_xgb, param_distributions=param_grid, n_iter=100,\n",
    "    scoring='accuracy', n_jobs=-1, cv=5, verbose=2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model with the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the XGBoost model with the best parameters\n",
    "best_model_xgb = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_xgb_proba = best_model_xgb.predict_proba(X_test)[:, 1]\n",
    "predictions_xgb = (predictions_xgb_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, predictions_xgb)\n",
    "print('XGBoost Accuracy with best parameters:', accuracy_xgb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260e999-42d9-4763-a795-adf3ab405d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "results_df_xgb = pd.DataFrame({\n",
    "    'True Values': y_test.values,\n",
    "    'Predictions': predictions_xgb\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path_xgb = 'xgboost_predictions_tuned.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df_xgb.to_excel(file_path_xgb, index=False)\n",
    "print(f'Results saved to {file_path_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5375337-8974-4ec4-8690-0e9778eaba5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f2985-c1f4-44b5-9ff6-2debd9cd3432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c46e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300,400,500,1000,2000],\n",
    "    'learning_rate': [0.01, 0.1,1.0],\n",
    "    'max_depth': [1,3, 5, 7,9,11]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_xgb = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'), param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_result_xgb = grid_xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(f\"Best: {grid_result_xgb.best_score_} using {grid_result_xgb.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262d18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02726b06-26cd-4553-a119-65f1397aebef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad85c68-8619-4f0c-a697-088bc9b3c5e5",
   "metadata": {},
   "source": [
    "# The combined models Class n Regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c584960-75d6-4282-8877-b29af8c910d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.01.02</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>1.27459</td>\n",
       "      <td>1.27459</td>\n",
       "      <td>1.27392</td>\n",
       "      <td>1.27392</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.01.02</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>1.27392</td>\n",
       "      <td>1.27410</td>\n",
       "      <td>1.27319</td>\n",
       "      <td>1.27409</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.01.02</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>1.27410</td>\n",
       "      <td>1.27537</td>\n",
       "      <td>1.27385</td>\n",
       "      <td>1.27475</td>\n",
       "      <td>772</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.01.02</td>\n",
       "      <td>01:30:00</td>\n",
       "      <td>1.27475</td>\n",
       "      <td>1.27528</td>\n",
       "      <td>1.27464</td>\n",
       "      <td>1.27506</td>\n",
       "      <td>307</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.01.02</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>1.27507</td>\n",
       "      <td>1.27508</td>\n",
       "      <td>1.27452</td>\n",
       "      <td>1.27468</td>\n",
       "      <td>1053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time     Open     High      Low    Close  Volume  Spread\n",
       "0  2019.01.02  00:00:00  1.27459  1.27459  1.27392  1.27392       3      76\n",
       "1  2019.01.02  00:30:00  1.27392  1.27410  1.27319  1.27409      43      60\n",
       "2  2019.01.02  01:00:00  1.27410  1.27537  1.27385  1.27475     772      13\n",
       "3  2019.01.02  01:30:00  1.27475  1.27528  1.27464  1.27506     307      10\n",
       "4  2019.01.02  02:00:00  1.27507  1.27508  1.27452  1.27468    1053       5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your dataset in a CSV file named 'Bittrex_BTCUSDT_d_sorted.csv'\n",
    "data = pd.read_excel('GBPUSD_M30_2019_01_2024_06_sorted_exc.xlsx')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8b6805-7434-4696-bc6b-217235710727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Daily_Change</th>\n",
       "      <th>Daily_Change_Percentage</th>\n",
       "      <th>...</th>\n",
       "      <th>Ichimoku_Conversion_Line</th>\n",
       "      <th>Ichimoku_Base_Line</th>\n",
       "      <th>Ichimoku_Leading_Span_A</th>\n",
       "      <th>Ichimoku_Leading_Span_B</th>\n",
       "      <th>Ichimoku_Lagging_Span</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>CMF</th>\n",
       "      <th>TSI</th>\n",
       "      <th>PSAR_Binary</th>\n",
       "      <th>Next_Day_Daily_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>1.27424</td>\n",
       "      <td>1.27445</td>\n",
       "      <td>1.27272</td>\n",
       "      <td>1.27343</td>\n",
       "      <td>2608</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.00129</td>\n",
       "      <td>-0.101199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274675</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27421</td>\n",
       "      <td>1.275330</td>\n",
       "      <td>-0.152910</td>\n",
       "      <td>-8.615436</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>1.27472</td>\n",
       "      <td>1.27544</td>\n",
       "      <td>1.27404</td>\n",
       "      <td>1.27423</td>\n",
       "      <td>2604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274675</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27348</td>\n",
       "      <td>1.275288</td>\n",
       "      <td>-0.201882</td>\n",
       "      <td>-8.272316</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>12:30:00</td>\n",
       "      <td>1.27542</td>\n",
       "      <td>1.27645</td>\n",
       "      <td>1.27512</td>\n",
       "      <td>1.27574</td>\n",
       "      <td>2305</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>0.118503</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274675</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27382</td>\n",
       "      <td>1.275303</td>\n",
       "      <td>-0.173135</td>\n",
       "      <td>-6.607499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>1.27744</td>\n",
       "      <td>1.27778</td>\n",
       "      <td>1.27703</td>\n",
       "      <td>1.27707</td>\n",
       "      <td>1492</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>0.104253</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274675</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27392</td>\n",
       "      <td>1.275339</td>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-4.128372</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>1.27534</td>\n",
       "      <td>1.27793</td>\n",
       "      <td>1.27507</td>\n",
       "      <td>1.27744</td>\n",
       "      <td>2473</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>0.028973</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274515</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.274952</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27388</td>\n",
       "      <td>1.275408</td>\n",
       "      <td>-0.124620</td>\n",
       "      <td>-1.872221</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time     Open     High      Low    Close  Volume  Spread  \\\n",
       "55 2019-01-08  15:30:00  1.27424  1.27445  1.27272  1.27343    2608       6   \n",
       "56 2019-01-08  15:00:00  1.27472  1.27544  1.27404  1.27423    2604       0   \n",
       "57 2019-01-08  12:30:00  1.27542  1.27645  1.27512  1.27574    2305       5   \n",
       "58 2019-01-08  14:00:00  1.27744  1.27778  1.27703  1.27707    1492       6   \n",
       "59 2019-01-08  13:30:00  1.27534  1.27793  1.27507  1.27744    2473       7   \n",
       "\n",
       "    Daily_Change  Daily_Change_Percentage  ...  Ichimoku_Conversion_Line  \\\n",
       "55      -0.00129                -0.101199  ...                  1.274675   \n",
       "56       0.00080                 0.062822  ...                  1.274675   \n",
       "57       0.00151                 0.118503  ...                  1.274675   \n",
       "58       0.00133                 0.104253  ...                  1.274675   \n",
       "59       0.00037                 0.028973  ...                  1.274515   \n",
       "\n",
       "    Ichimoku_Base_Line  Ichimoku_Leading_Span_A  Ichimoku_Leading_Span_B  \\\n",
       "55             1.27539                 1.275033                  1.27539   \n",
       "56             1.27539                 1.275033                  1.27539   \n",
       "57             1.27539                 1.275033                  1.27539   \n",
       "58             1.27539                 1.275033                  1.27539   \n",
       "59             1.27539                 1.274952                  1.27539   \n",
       "\n",
       "    Ichimoku_Lagging_Span      VWAP       CMF       TSI  PSAR_Binary  \\\n",
       "55                1.27421  1.275330 -0.152910 -8.615436            1   \n",
       "56                1.27348  1.275288 -0.201882 -8.272316            1   \n",
       "57                1.27382  1.275303 -0.173135 -6.607499            0   \n",
       "58                1.27392  1.275339 -0.212891 -4.128372            0   \n",
       "59                1.27388  1.275408 -0.124620 -1.872221            0   \n",
       "\n",
       "    Next_Day_Daily_Change  \n",
       "55                0.00080  \n",
       "56                0.00151  \n",
       "57                0.00133  \n",
       "58                0.00037  \n",
       "59               -0.00210  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort data by Date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate daily change and percentage change\n",
    "data['Daily_Change'] = data['Close'].diff()\n",
    "data['Daily_Change_Percentage'] = data['Daily_Change'] / data['Close'].shift(1) * 100\n",
    "\n",
    "# Drop the first row with NaN values from the shift operation\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Add direction column\n",
    "data['Direction'] = data['Daily_Change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Create additional features based on technical indicators\n",
    "def add_indicators(df):\n",
    "    df['SMA_21'] = df['Close'].rolling(window=21).mean()\n",
    "    df['SMA_55'] = df['Close'].rolling(window=55).mean()\n",
    "    df['EMA_13'] = df['Close'].ewm(span=34, adjust=False).mean()\n",
    "    df['EMA_89'] = df['Close'].ewm(span=89, adjust=False).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'], 14)\n",
    "    df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = compute_macd(df['Close'])\n",
    "    df['Bollinger_Upper'], df['Bollinger_Lower'] = compute_bollinger_bands(df['Close'])\n",
    "    df['ATR'] = compute_atr(df['High'], df['Low'], df['Close'], 14)\n",
    "    df['Stochastic_K'], df['Stochastic_D'] = compute_stochastic(df['High'], df['Low'], df['Close'])\n",
    "    df['Williams_%R'] = compute_williams_r(df['High'], df['Low'], df['Close'])\n",
    "    df['PSAR'] = psar(df['High'], df['Low'])\n",
    "    df['CCI'] = compute_cci(df['High'], df['Low'], df['Close'], 20)\n",
    "    ichimoku = compute_ichimoku(df['High'], df['Low'], df['Close'])\n",
    "    df['Ichimoku_Conversion_Line'] = ichimoku['Conversion_Line']\n",
    "    df['Ichimoku_Base_Line'] = ichimoku['Base_Line']\n",
    "    df['Ichimoku_Leading_Span_A'] = ichimoku['Leading_Span_A']\n",
    "    df['Ichimoku_Leading_Span_B'] = ichimoku['Leading_Span_B']\n",
    "    df['Ichimoku_Lagging_Span'] = ichimoku['Lagging_Span']\n",
    "    df['VWAP'] = compute_vwap(df['Close'], df['Volume'])\n",
    "    #df['OBV'] = compute_obv(df['Close'], df['Volume'])\n",
    "    df['CMF'] = compute_cmf(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "    df['TSI'] = compute_tsi(df['Close'])\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period):\n",
    "    delta = series.diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, short_period=12, long_period=26, signal_period=9):\n",
    "    short_ema = series.ewm(span=short_period, adjust=False).mean()\n",
    "    long_ema = series.ewm(span=long_period, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_period, adjust=False).mean()\n",
    "    hist = macd - signal\n",
    "    return macd, signal, hist\n",
    "\n",
    "def compute_bollinger_bands(series, window=21, no_of_std=2):\n",
    "    rolling_mean = series.rolling(window).mean()\n",
    "    rolling_std = series.rolling(window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * no_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * no_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def compute_atr(high, low, close, period):\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    true_range = pd.DataFrame({'TR1': tr1, 'TR2': tr2, 'TR3': tr3}).max(axis=1)\n",
    "    return true_range.rolling(window=period).mean()\n",
    "\n",
    "def compute_stochastic(high, low, close, k_period=14, d_period=3):\n",
    "    low_min = low.rolling(window=k_period).min()\n",
    "    high_max = high.rolling(window=k_period).max()\n",
    "    stoch_k = 100 * ((close - low_min) / (high_max - low_min))\n",
    "    stoch_d = stoch_k.rolling(window=d_period).mean()\n",
    "    return stoch_k, stoch_d\n",
    "\n",
    "def compute_williams_r(high, low, close, period=14):\n",
    "    highest_high = high.rolling(window=period).max()\n",
    "    lowest_low = low.rolling(window=period).min()\n",
    "    williams_r = (highest_high - close) / (highest_high - lowest_low) * -100\n",
    "    return williams_r\n",
    "    \n",
    "def psar(high, low):\n",
    "    psar = [0] * len(high)\n",
    "    psar[0] = low.iloc[0]  # Use .iloc instead of direct indexing\n",
    "    acceleration = 0.02\n",
    "    max_acceleration = 0.2\n",
    "    for i in range(1, len(high)):\n",
    "        if high.iloc[i] > psar[i-1]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = psar[i-1] + acceleration * (high.iloc[i] - psar[i-1])  # Use .iloc instead of direct indexing\n",
    "        else:\n",
    "            psar[i] = psar[i-1] - acceleration * (psar[i-1] - low.iloc[i])  # Use .iloc instead of direct indexing\n",
    "        if psar[i] > high.iloc[i]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = high.iloc[i]\n",
    "        elif psar[i] < low.iloc[i]:  # Use .iloc instead of direct indexing\n",
    "            psar[i] = low.iloc[i]\n",
    "        acceleration = min(max_acceleration, acceleration + 0.02)\n",
    "    return psar\n",
    "    \n",
    "\n",
    "def compute_cci(high, low, close, period=20):\n",
    "    tp = (high + low + close) / 3\n",
    "    tp_sma = tp.rolling(window=period).mean()\n",
    "    mad = tp.rolling(window=period).apply(lambda x: np.mean(np.abs(x - np.mean(x))))\n",
    "    cci = (tp - tp_sma) / (0.015 * mad)\n",
    "    return cci\n",
    "\n",
    "def compute_ichimoku(high, low, close):\n",
    "    nine_period_high = high.rolling(window=9).max()\n",
    "    nine_period_low = low.rolling(window=9).min()\n",
    "    period26_high = high.rolling(window=26).max()\n",
    "    period26_low = low.rolling(window=26).min()\n",
    "    period52_high = high.rolling(window=52).max()\n",
    "    period52_low = low.rolling(window=52).min()\n",
    "    ichimoku_cloud = {\n",
    "        'Conversion_Line': (nine_period_high + nine_period_low) / 2,\n",
    "        'Base_Line': (period26_high + period26_low) / 2,\n",
    "        'Leading_Span_A': ((nine_period_high + nine_period_low) / 2 + (period26_high + period26_low) / 2) / 2,\n",
    "        'Leading_Span_B': (period52_high + period52_low) / 2,\n",
    "        'Lagging_Span': close.shift(-26)\n",
    "    }\n",
    "    return ichimoku_cloud\n",
    "\n",
    "def compute_vwap(close, volume):\n",
    "    return (close * volume).cumsum() / volume.cumsum()\n",
    "\n",
    "def compute_obv(close, volume):\n",
    "    obv = volume.copy()\n",
    "    obv[1:] = np.where(close.iloc[1:] > close.iloc[:-1], volume.iloc[1:], np.where(close.iloc[1:] < close.iloc[:-1], -volume.iloc[1:], 0))\n",
    "    return obv.cumsum()\n",
    "\n",
    "def compute_cmf(high, low, close, volume, period=20):\n",
    "    mfv = ((close - low) - (high - close)) / (high - low) * volume\n",
    "    cmf = mfv.rolling(window=period).sum() / volume.rolling(window=period).sum()\n",
    "    return cmf\n",
    "\n",
    "def compute_tsi(close, r=25, s=13):\n",
    "    m25 = close.diff(1)\n",
    "    abs_m25 = abs(m25)\n",
    "    m25s = m25.ewm(span=r, adjust=False).mean()\n",
    "    abs_m25s = abs_m25.ewm(span=r, adjust=False).mean()\n",
    "    m25s = m25s.ewm(span=s, adjust=False).mean()\n",
    "    abs_m25s = abs_m25s.ewm(span=s, adjust=False).mean()\n",
    "    tsi = m25s / abs_m25s * 100\n",
    "    return tsi\n",
    "    \n",
    "# Shift the daily change to get the next day's change as the action for the current day\n",
    "data['Next_Day_Daily_Change'] = data['Daily_Change'].shift(-1)\n",
    "\n",
    "# Drop the last row with NaN values in 'Next_Day_Daily_Change'\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Add action column based on the next day's daily change\n",
    "def determine_action(daily_change):\n",
    "    if daily_change > 0:\n",
    "        return 1\n",
    "    elif daily_change < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "data['Action'] = data['Next_Day_Daily_Change'].apply(determine_action)\n",
    "    \n",
    "# Apply the indicators\n",
    "data = add_indicators(data)\n",
    "\n",
    "# Drop rows with NaN values created by rolling windows\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert PSAR to binary values based on Close prices\n",
    "data['PSAR_Binary'] = data.apply(lambda row: 1 if row['PSAR'] > row['Close'] else 0, axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e7e6fc-7323-4766-8300-9fab21b26fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target, shift the features by 1 day\n",
    "X = data[['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_21', 'SMA_55', 'EMA_13', 'EMA_89', 'RSI', 'MACD', \n",
    "            'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Stochastic_K', \n",
    "            'Stochastic_D', 'Williams_%R', 'CCI', 'Ichimoku_Conversion_Line', 'Ichimoku_Base_Line', \n",
    "            'Ichimoku_Leading_Span_A', 'Ichimoku_Leading_Span_B', 'Ichimoku_Lagging_Span', 'PSAR', \n",
    "            'VWAP', 'CMF', 'TSI', 'Daily_Change_Percentage', 'PSAR_Binary', 'Next_Day_Daily_Change']].shift(1)\n",
    "y_class = data['Action']\n",
    "y_reg = data['Daily_Change']\n",
    "\n",
    "# Drop the rows with NaN values created by shifting\n",
    "X.dropna(inplace=True)\n",
    "y_class = y_class.iloc[1:]\n",
    "y_reg = y_reg.iloc[1:]\n",
    "\n",
    "#X.fillna(0, inplace=True)\n",
    "\n",
    "#y_class = y_class.iloc[X.index]\n",
    "#y_reg = y_reg.iloc[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef69faf4-db26-418e-9c28-ae3781c620ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_21</th>\n",
       "      <th>SMA_55</th>\n",
       "      <th>EMA_13</th>\n",
       "      <th>EMA_89</th>\n",
       "      <th>RSI</th>\n",
       "      <th>...</th>\n",
       "      <th>Ichimoku_Leading_Span_A</th>\n",
       "      <th>Ichimoku_Leading_Span_B</th>\n",
       "      <th>Ichimoku_Lagging_Span</th>\n",
       "      <th>PSAR</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>CMF</th>\n",
       "      <th>TSI</th>\n",
       "      <th>Daily_Change_Percentage</th>\n",
       "      <th>PSAR_Binary</th>\n",
       "      <th>Next_Day_Daily_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.27424</td>\n",
       "      <td>1.27445</td>\n",
       "      <td>1.27272</td>\n",
       "      <td>1.27343</td>\n",
       "      <td>2608.0</td>\n",
       "      <td>1.275880</td>\n",
       "      <td>1.275302</td>\n",
       "      <td>1.275438</td>\n",
       "      <td>1.275297</td>\n",
       "      <td>40.567854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27421</td>\n",
       "      <td>1.274042</td>\n",
       "      <td>1.275330</td>\n",
       "      <td>-0.152910</td>\n",
       "      <td>-8.615436</td>\n",
       "      <td>-0.101199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.27472</td>\n",
       "      <td>1.27544</td>\n",
       "      <td>1.27404</td>\n",
       "      <td>1.27423</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>1.275760</td>\n",
       "      <td>1.275290</td>\n",
       "      <td>1.275369</td>\n",
       "      <td>1.275273</td>\n",
       "      <td>44.439095</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27348</td>\n",
       "      <td>1.274322</td>\n",
       "      <td>1.275288</td>\n",
       "      <td>-0.201882</td>\n",
       "      <td>-8.272316</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.27542</td>\n",
       "      <td>1.27645</td>\n",
       "      <td>1.27512</td>\n",
       "      <td>1.27574</td>\n",
       "      <td>2305.0</td>\n",
       "      <td>1.275625</td>\n",
       "      <td>1.275312</td>\n",
       "      <td>1.275390</td>\n",
       "      <td>1.275283</td>\n",
       "      <td>49.676226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27382</td>\n",
       "      <td>1.275120</td>\n",
       "      <td>1.275303</td>\n",
       "      <td>-0.173135</td>\n",
       "      <td>-6.607499</td>\n",
       "      <td>0.118503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.27744</td>\n",
       "      <td>1.27778</td>\n",
       "      <td>1.27703</td>\n",
       "      <td>1.27707</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>1.275649</td>\n",
       "      <td>1.275365</td>\n",
       "      <td>1.275486</td>\n",
       "      <td>1.275323</td>\n",
       "      <td>52.592593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.275033</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27392</td>\n",
       "      <td>1.277030</td>\n",
       "      <td>1.275339</td>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-4.128372</td>\n",
       "      <td>0.104253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.27534</td>\n",
       "      <td>1.27793</td>\n",
       "      <td>1.27507</td>\n",
       "      <td>1.27744</td>\n",
       "      <td>2473.0</td>\n",
       "      <td>1.275713</td>\n",
       "      <td>1.275407</td>\n",
       "      <td>1.275598</td>\n",
       "      <td>1.275370</td>\n",
       "      <td>48.896195</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274952</td>\n",
       "      <td>1.27539</td>\n",
       "      <td>1.27388</td>\n",
       "      <td>1.277210</td>\n",
       "      <td>1.275408</td>\n",
       "      <td>-0.124620</td>\n",
       "      <td>-1.872221</td>\n",
       "      <td>0.028973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.00210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open     High      Low    Close  Volume    SMA_21    SMA_55    EMA_13  \\\n",
       "56  1.27424  1.27445  1.27272  1.27343  2608.0  1.275880  1.275302  1.275438   \n",
       "57  1.27472  1.27544  1.27404  1.27423  2604.0  1.275760  1.275290  1.275369   \n",
       "58  1.27542  1.27645  1.27512  1.27574  2305.0  1.275625  1.275312  1.275390   \n",
       "59  1.27744  1.27778  1.27703  1.27707  1492.0  1.275649  1.275365  1.275486   \n",
       "60  1.27534  1.27793  1.27507  1.27744  2473.0  1.275713  1.275407  1.275598   \n",
       "\n",
       "      EMA_89        RSI  ...  Ichimoku_Leading_Span_A  \\\n",
       "56  1.275297  40.567854  ...                 1.275033   \n",
       "57  1.275273  44.439095  ...                 1.275033   \n",
       "58  1.275283  49.676226  ...                 1.275033   \n",
       "59  1.275323  52.592593  ...                 1.275033   \n",
       "60  1.275370  48.896195  ...                 1.274952   \n",
       "\n",
       "    Ichimoku_Leading_Span_B  Ichimoku_Lagging_Span      PSAR      VWAP  \\\n",
       "56                  1.27539                1.27421  1.274042  1.275330   \n",
       "57                  1.27539                1.27348  1.274322  1.275288   \n",
       "58                  1.27539                1.27382  1.275120  1.275303   \n",
       "59                  1.27539                1.27392  1.277030  1.275339   \n",
       "60                  1.27539                1.27388  1.277210  1.275408   \n",
       "\n",
       "         CMF       TSI  Daily_Change_Percentage  PSAR_Binary  \\\n",
       "56 -0.152910 -8.615436                -0.101199          1.0   \n",
       "57 -0.201882 -8.272316                 0.062822          1.0   \n",
       "58 -0.173135 -6.607499                 0.118503          0.0   \n",
       "59 -0.212891 -4.128372                 0.104253          0.0   \n",
       "60 -0.124620 -1.872221                 0.028973          0.0   \n",
       "\n",
       "    Next_Day_Daily_Change  \n",
       "56                0.00080  \n",
       "57                0.00151  \n",
       "58                0.00133  \n",
       "59                0.00037  \n",
       "60               -0.00210  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b34d8b-1cd4-42a3-ad50-c3947e6b7bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56    1\n",
       "57    1\n",
       "58    1\n",
       "59    0\n",
       "60    0\n",
       "Name: Action, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8399d9dd-da85-420e-8d14-9741f0a95c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.58\n",
      "Logistic Regression Test Accuracy: 0.55\n",
      "Decision Tree Train Accuracy: 0.65\n",
      "Decision Tree Test Accuracy: 0.64\n",
      "Random Forest Train Accuracy: 0.65\n",
      "Random Forest Test Accuracy: 0.65\n",
      "XGBoost Train Accuracy: 0.71\n",
      "XGBoost Test Accuracy: 0.66\n",
      "Gradient Boosting Train Accuracy: 0.67\n",
      "Gradient Boosting Test Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_class, test_size=0.1, shuffle=False)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.1, shuffle=False)\n",
    "\n",
    "# Train classifiers and get their predictions\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=500, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=2000, learning_rate=0.01, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', random_state=42, eval_metric='logloss'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=2, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "}\n",
    "#CatBoostClassifier: AUC Score=0.511 AdaBoostClassifier\n",
    "\n",
    "class_preds_train = pd.DataFrame(index=X_train.index)\n",
    "class_preds_test = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train_class)\n",
    "    class_preds_train[name] = clf.predict(X_train)\n",
    "    class_preds_test[name] = clf.predict(X_test)\n",
    "    \n",
    "    # Print the accuracy for each classifier\n",
    "    train_accuracy = accuracy_score(y_train_class, class_preds_train[name])\n",
    "    test_accuracy = accuracy_score(y_test_class, class_preds_test[name])\n",
    "    print(f'{name} Train Accuracy: {train_accuracy:.2f}')\n",
    "    print(f'{name} Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5e615e-f09d-4be2-bdc9-ba407deb52e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: MSE = 1.2144696656681133e-08\n",
      "XGBoost: MSE = 1.5696083607647845e-08\n",
      "Gradient Boosting: MSE = 1.8581104593459692e-10\n",
      "Results saved to combo_models4thi.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Add the classification predictions as features for regression\n",
    "X_train_reg = pd.concat([X_train.reset_index(drop=True), class_preds_train.reset_index(drop=True)], axis=1)\n",
    "X_test_reg = pd.concat([X_test.reset_index(drop=True), class_preds_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Align the target variable with the training and testing sets\n",
    "y_train_reg = y_train_reg.iloc[:len(X_train_reg)]\n",
    "y_test_reg = y_test_reg.iloc[:len(X_test_reg)]\n",
    "\n",
    "# Reshape data for CNN and LSTM\n",
    "X_train_seq = np.expand_dims(X_train_reg.values, axis=1)\n",
    "X_test_seq = np.expand_dims(X_test_reg.values, axis=1)\n",
    "\n",
    "# Define and train traditional regression models\n",
    "regressors = {\n",
    "    'Linear Regression': XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.01),\n",
    "    'XGBoost': XGBRegressor(n_estimators=2000, max_depth=1, learning_rate=0.01),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=500, max_depth=5, learning_rate=0.01)\n",
    "}\n",
    "\n",
    "reg_metrics = {}\n",
    "confidence_scores = []\n",
    "\n",
    "for name, reg in regressors.items():\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    y_pred_reg = reg.predict(X_test_reg)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    reg_metrics[name] = mse\n",
    "    \n",
    "    # Calculate confidence score using ensemble predictions\n",
    "    y_pred_ensemble = []\n",
    "    for i in range(10):  # Ensemble size\n",
    "        reg.fit(X_train_reg, y_train_reg)\n",
    "        y_pred_ensemble.append(reg.predict(X_test_reg))\n",
    "    y_pred_ensemble = np.array(y_pred_ensemble)\n",
    "    mean_preds = y_pred_ensemble.mean(axis=0)\n",
    "    std_preds = y_pred_ensemble.std(axis=0)\n",
    "    confidence_scores.append(std_preds)\n",
    "\n",
    "# Print regression metrics\n",
    "for name, mse in reg_metrics.items():\n",
    "    print(f'{name}: MSE = {mse}')\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df_xgb = pd.DataFrame({\n",
    "    'True Values': y_test_reg.values,\n",
    "    'Predictions': y_pred_reg,\n",
    "    'Confidence Scores': np.mean(confidence_scores, axis=0)  # Average confidence scores across models\n",
    "})\n",
    "\n",
    "# Define the file path\n",
    "file_path_xgb = 'combo_models4thi.xlsx'\n",
    "\n",
    "# Save to Excel\n",
    "results_df_xgb.to_excel(file_path_xgb, index=True)\n",
    "print(f'Results saved to {file_path_xgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee5ac9-7cf1-4c25-a857-416e297eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train CNN and LSTM models\n",
    "cnn_model.fit(X_train_seq, y_train_reg, epochs=50, verbose=1)\n",
    "lstm_model.fit(X_train_seq, y_train_reg, epochs=50, verbose=1)\n",
    "\n",
    "# Predict and evaluate CNN and LSTM models\n",
    "y_pred_cnn = cnn_model.predict(X_test_seq)\n",
    "y_pred_lstm = lstm_model.predict(X_test_seq)\n",
    "\n",
    "cnn_mse = mean_squared_error(y_test_reg, y_pred_cnn)\n",
    "lstm_mse = mean_squared_error(y_test_reg, y_pred_lstm)\n",
    "\n",
    "print(f'CNN: MSE = {cnn_mse}')\n",
    "print(f'LSTM: MSE = {lstm_mse}')\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_reg.index, y_test_reg, label='True Daily Change')\n",
    "plt.plot(y_test_reg.index, regressors['XGBoost'].predict(X_test_reg), label='XGBoost Predicted Daily Change')\n",
    "plt.plot(y_test_reg.index, y_pred_cnn, label='CNN Predicted Daily Change')\n",
    "plt.plot(y_test_reg.index, y_pred_lstm, label='LSTM Predicted Daily Change')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Change')\n",
    "plt.title('True vs Predicted Daily Change')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ece04d-5fcd-4018-92f3-25918c294d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:(i + seq_length)].values)\n",
    "        y.append(data.iloc[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 10\n",
    "X_lstm, y_lstm = create_sequences(data[X.columns.tolist() + ['Direction']], seq_length)\n",
    "y_lstm_reg = data['Daily_Change_Percentage'][seq_length:]\n",
    "\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm, y_train_lstm_reg, y_test_lstm_reg = train_test_split(\n",
    "    X_lstm, y_lstm, y_lstm_reg, test_size=0.2, shuffle=False)\n",
    "\n",
    "# LSTM Model for Classification\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(seq_length, X_train_lstm.shape[2])),\n",
    "    LSTM(50),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_lstm = model_lstm.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_split=0.2)\n",
    "lstm_accuracy = model_lstm.evaluate(X_test_lstm, y_test_lstm)[1]\n",
    "print(f'LSTM Accuracy: {lstm_accuracy:.2f}')\n",
    "\n",
    "# CNN Model for Classification\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(seq_length, X_train_lstm.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_cnn = model_cnn.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_split=0.2)\n",
    "cnn_accuracy = model_cnn.evaluate(X_test_lstm, y_test_lstm)[1]\n",
    "print(f'CNN Accuracy: {cnn_accuracy:.2f}')\n",
    "\n",
    "# Visualization of Deep Learning Models Training History\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_lstm.history['accuracy'], label='LSTM Train Accuracy')\n",
    "plt.plot(history_lstm.history['val_accuracy'], label='LSTM Val Accuracy')\n",
    "plt.title('LSTM Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cnn.history['accuracy'], label='CNN Train Accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='CNN Val Accuracy')\n",
    "plt.title('CNN Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb629c8-4920-40ab-ae14-4172c2022f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5709e15-a0bc-4b74-8354-066fc6f9b3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81088838-58e4-48bc-bb95-2a820f116a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d981131-896b-45e9-9c3a-5845db3024f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5563ae4-ad7a-4612-94b4-184a2baf47cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603b783-8f77-4f6a-944f-0faab2fb169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9139a7e-ad7c-4234-b417-4db1891c099d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc69d83-213d-49c9-a451-34bcf4857f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc5ccd-0123-4556-88cd-afe576f50d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Fine-tune the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea31e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def build_model(units=50, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
    "    model.add(LSTM(units=units, return_sequences=False))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=build_model, epochs=50, batch_size=32, verbose=2)\n",
    "param_grid = {\n",
    "    'units': [50, 100],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train_lstm, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Deploy and Monitor the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b43e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model_lstm.save('best_lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de637f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Use the Model for Predictions\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('best_lstm_model.h5')\n",
    "\n",
    "# Function to make daily predictions\n",
    "def make_prediction(new_data):\n",
    "    new_data = np.reshape(new_data.values, (new_data.shape[0], 1, new_data.shape[1]))\n",
    "    prediction = model.predict(new_data)\n",
    "    return prediction\n",
    "\n",
    "# Example: Predicting the next day's change\n",
    "new_data = X_test.iloc[-1:]  # Replace with new daily data\n",
    "predicted_change = make_prediction(new_data)\n",
    "print(f\"Predicted Daily Change Percentage: {predicted_change[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ce864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuously Monitor and Update the Model\n",
    "# You can set up a script to periodically fetch new data, update your dataset, retrain the model, and save the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a88b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrain the model\n",
    "def retrain_model(new_data):\n",
    "    global data, X, y, X_train, X_test, y_train, y_test, X_train_lstm, X_test_lstm\n",
    "    \n",
    "    # Append new data to the existing dataset\n",
    "    data = data.append(new_data, ignore_index=True)\n",
    "    data.dropna(inplace=True)  # Ensure no NaN values\n",
    "    \n",
    "    # Update features and target\n",
    "    X = data[['SMA_5', 'SMA_10', 'EMA_5', 'EMA_10', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'Bollinger_Upper', 'Bollinger_Lower', 'ATR', 'Volume_BTC', 'Volume_USDT']]\n",
    "    y = data['Daily_Change_Percentage']\n",
    "    \n",
    "    # Split the updated data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Reshape input data for LSTM\n",
    "    X_train_lstm = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_lstm = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # Retrain the model\n",
    "    model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_lstm, y_test), verbose=2)\n",
    "    \n",
    "    # Save the updated model\n",
    "    model_lstm.save('best_lstm_model.h5')\n",
    "\n",
    "# Example: Retraining with new daily data\n",
    "new_daily_data = {\n",
    "    'Unix': 1.5031E+12,\n",
    "    'Date': '8/20/2017',\n",
    "    'Symbol': 'BTCUSDT',\n",
    "    'Open': 4139.98,\n",
    "    'High': 4200,\n",
    "    'Low': 4100,\n",
    "    'Close': 4150,\n",
    "    'Volume BTC': 400,\n",
    "    'Volume USDT': 1600000,\n",
    "    'tradecount': 2200,\n",
    "    'Daily_Change': 10,\n",
    "    'Daily_Change_Percentage': 0.24\n",
    "}\n",
    "new_daily_data = pd.DataFrame([new_daily_data])\n",
    "retrain_model(new_daily_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74173f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd5bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba768597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
